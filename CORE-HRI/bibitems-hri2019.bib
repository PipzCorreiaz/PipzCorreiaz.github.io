@inproceedings{10.5555/3378680.3378684,
author = {Hu, Yuhan and Hoffman, Guy},
title = {Using Skin Texture Change to Design Emotion Expression in Social Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {We evaluate the emotional expression capacity of skin texture change, a new design modality for social robots. In contrast to the majority of robots that use gestures and facial movements to express internal states, we developed an emotionally expressive robot that communicates using dynamically changing skin textures. The robot's shell is covered in actuated goosebumps and spikes, with programmable frequency and amplitude patterns. In a controlled study (n = 139) we presented eight texture patterns to participants in three interaction modes: online video viewing, in person observation, and touching the texture. For most of the explored texture patterns, participants consistently perceived them as expressing specific emotions, with similar distributions across all three modes. This indicates that a texture changing skin can be a useful new tool for robot designers. Based on the specific texture-to-emotion mappings, we provide actionable design implications, recommending using the shape of a texture to communicate emotional valence, and the frequency of texture movement to convey emotional arousal. Given that participants were most sensitive to valence when touching the texture, and were also most confident in their ratings using that mode, we conclude that touch is a promising design channel for human-robot communication.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {2–10},
numpages = {9},
keywords = {texture-change, emotion expression, empirical study, human-robot interaction, soft robotics, nonverbal behavior},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378685,
author = {Thiessen, Raquel and Rea, Daniel J. and Garcha, Diljot S. and Cheng, Cheng and Young, James E.},
title = {Infrasound for HRI: A Robot Using Low-Frequency Vibrations to Impact How People Perceive Its Actions},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {We investigate robots using infrasound, low-frequency vibrational energy at or near the human hearing threshold, as an interaction tool for working with people. Research in psychology suggests that the presence of infrasound can impact a person's emotional state and mood, even when the person is not acutely aware of the infrasound. Although often not noticed, infrasound is commonly present in many situations including factories, airports, or near motor vehicles. Further, a robot itself can produce infrasound. Thus, we examine if infrasound may impact how people interpret a robot's social communication: if the presence of infrasound makes a robot seem more or less happy, energetic, etc., as a result of impacting a person's mood. We present the results from a series of experiments that investigate how people rate a social robot's emotionally-charged gestures, and how varied levels and sources of infrasound impact these ratings. Our results show that infrasound does have a psychological effect on the person's perception of the robot's behaviors, supporting this as a technique that a robot can use as part of its interaction design toolkit. We further provide a comparison of infrasound generation methods.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {11–18},
numpages = {8},
keywords = {infrasound, social HRI, sonic interaction},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378686,
author = {Rea, Daniel J. and Young, James E.},
title = {Backseat Teleoperator: Affective Feedback with on-Screen Agents to Influence Teleoperation},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {We investigate if an on-screen agent that reacts to a teleoperator's driving performance (e.g., by showing fear during poor driving) can influence teleoperation. Serving as a kind of virtual passenger, we explore if and how this agent's reactions may impact teleoperation. Our design concept is to create an emotional response in the operator (e.g., to feel bad for the agent), with the ultimate goal of shaping driving behavior (e.g., to slow down to calm the agent). We designed and implemented two proof-of-concept agent personas that react differently to operator driving. By conducting an initial proof-of-concept study comparing our agents to a base case, we were able to observe the impact of our agent personas on operator experience, perception of the robot, and driving behavior. While our results failed to find compelling evidence of changed teleoperator behavior, we did demonstrate that emotional on-screen agents can alter teleoperator emotion. Our initial results support the plausibility of passenger agents for impacting teleoperation, and highlight potential for more targeted, ongoing work in applying social techniques to teleoperation interfaces.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {19–28},
numpages = {10},
keywords = {social interfaces, human-robot interaction, teleoperation},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378687,
author = {Fischer, Kerstin and Jung, Malte and Jensen, Lars Christian and Wieschen, Maria Vanessa aus der},
title = {Emotion Expression in HRI: When and Why},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In this paper, we draw attention to the social functions of emotional display in interaction. A review of HRI papers on emotion suggests that this perspective is rarely taken in the field, but that it is useful to account for the context-and culture-dependency of emotional expression. We show in two case studies that emotional display is expected to occur at very specific places in interaction and rather independently from general emotional states, and that different cultures have different conventions regarding emotional expression. Based on conversation analytic work and the results from our case studies, we present design recommendations which allow the implementation of specific emotional signals for different human-robot interaction situations.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {29–38},
numpages = {10},
keywords = {human-robot interaction, situation-specificity, conversation analysis, emotion expression},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378689,
author = {Xie, Yaqi and Bodala, Indu P and Ong, Desmond C. and Hsu, David and Soh, Harold},
title = {Robot Capability and Intention in Trust-Based Decisions across Tasks},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In this paper, we present results from a human-subject study designed to explore two facets of human mental models of robots---inferred capability and intention---and their relationship to overall trust and eventual decisions. In particular, we examine delegation situations characterized by uncertainty, and explore how inferred capability and intention are applied across different tasks. We develop an online survey where human participants decide whether to delegate control to a simulated UAV agent. Our study shows that human estimations of robot capability and intent correlate strongly with overall self-reported trust. However, overall trust is not independently sufficient to determine whether a human will decide to trust (delegate) a given task to a robot. Instead, our study reveals that estimations of robot intention, capability, and overall trust are integrated when deciding to delegate. From a broader perspective, these results suggest that calibrating overall trust alone is insufficient; to make correct decisions, humans need (and use) multi-faceted mental models when collaborating with robots across multiple contexts.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {39–47},
numpages = {9},
keywords = {trust, intention, capability, human robot collaboration},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378690,
author = {Geiskkovitch, Denise Y. and Thiessen, Raquel and Young, James E. and Glenwright, Melanie R.},
title = {What? That's Not a Chair!: How Robot Informational Errors Affect Children's Trust towards Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Robots that interact with children are becoming more common in places such as child care and hospital environments. While such robots may mistakenly provide nonsensical information, or have mechanical malfunctions, we know little of how these robot errors are perceived by children, and how they impact trust. This is particularly important when robots provide children with information or instructions, such as in education or health care. Drawing inspiration from established psychology literature investigating how children trust entities who teach or provide them with information (informants), we designed and conducted an experiment to examine how robot errors affect how young children (3-5 years old) trust robots. Our results suggest that children utilize their understanding of people to develop their perceptions of robots, and use this to determine how to interact with robots. Specifically, we found that children developed their trust model of a robot based on the robot's previous errors, similar to how they would for a person. We however failed to replicate other prior findings with robots. Our results provide insight into how children as young as 3 years old might perceive robot errors and develop trust.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {48–56},
numpages = {9},
keywords = {trust, child-robot interaction, robot errors},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378691,
author = {Sebo, Sarah Strohkorb and Krishnamurthi, Priyanka and Scassellati, Brian},
title = {"I Don't Believe You": Investigating the Effects of Robot Trust Violation and Repair},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {When a robot breaks a person's trust by making a mistake or failing, continued interaction will depend heavily on how the robot repairs the trust that was broken. Prior work in psychology has demonstrated that both the trust violation framing and the trust repair strategy influence how effectively trust can be restored. We investigate trust repair between a human and a robot in the context of a competitive game, where a robot tries to restore a human's trust after a broken promise, using either a competence or integrity trust violation framing and either an apology or denial trust repair strategy. Results from a 2\texttimes{}2 between-subjects study (n = 82) show that participants interacting with a robot employing the integrity trust violation framing and the denial trust repair strategy are significantly more likely to exhibit behavioral retaliation toward the robot. In the Dyadic Trust Scale survey, an interaction between trust violation framing and trust repair strategy was observed. Our results demonstrate the importance of considering both trust violation framing and trust repair strategy choice when designing robots to repair trust. We also discuss the influence of human-to-robot promises and ethical considerations when framing and repairing trust between a human and robot.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {57–65},
numpages = {9},
keywords = {trust repair, human-robot interaction, trust},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378692,
author = {Hedaoo, Samarendra and Williams, Akim and Wadgaonkar, Chinmay and Knight, Heather},
title = {A Robot Barista Comments on Its Clients: Social Attitudes toward Robot Data Use},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper explores peoples attitudes about a service robot using customer data in conversation. In particular, how can robots understand privacy expectations in social grey-areas like cafes, which are both open to the public and used for private meetings? To answer this question, we introduce the Theater Method, which allows a participant to experience a "violation" of their privacy rather than have their actual privacy be violated. Using Python to generate 288 scripts that fully explored our research variables, we ran a large-scale online study (N=4608). To validate our results and ask more in-depth questions, we also ran an in-person follow-up (N=20). The experiments explored social &amp; data-inspired variables such as data source, the positive or negative use of that data, and whom the robot verbally addressed, all of which significantly predicted participants' social attitudes towards the robot's politeness, consideration, appropriateness, and respect of privacy. Body language analysis and cafe-related conversation were the lowest risk, but even more extreme data channels are potentially okay when used for positive purposes.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {66–74},
numpages = {9},
keywords = {data security, human-robot interaction, robotics, privacy, service robots, social robotics},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378694,
author = {Welfare, Katherine S. and Hallowell, Matthew R. and Shah, Julie A. and Riek, Laurel D.},
title = {Consider the Human Work Experience When Integrating Robotics in the Workplace},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Worldwide, manufacturers are reimagining the future of their workforce and its connection to technology. Rather than replacing humans, Industry 5.0 explores how humans and robots can best complement one another's unique strengths. However, realizing this vision requires an in-depth understanding of how workers view the positive and negative attributes of their jobs, and the place of robots within it. In this paper, we explore the relationship between work attributes and automation goals by engaging in field research at a manufacturing plant. We conducted 50 face-to-face interviews with assembly-line workers (n = 50), which we analyzed using discourse analysis and social constructivist methods. We found that the work attributes deemed most positive by participants include social interaction, movement and exercise, (human) autonomy, problem solving, task variety, and building with their hands. The main negative work attributes included health and safety issues, feeling rushed, and repetitive work. We identified several ways robots could help reduce negative work attributes and enhance positive ones, such as reducing work interruptions and cultivating physical and psychological well-being. Based on our findings, we created a set of integration considerations for organizations planning to deploy robotics technology, and discuss how the manufacturing and HRI communities can explore these ideas in the future.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {75–84},
numpages = {10},
keywords = {work attributes, collaborative robotics, manufacturing, Industry 5.0, human robot collaboration, future of work},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378695,
author = {Brooks, Connor and Szafir, Daniel},
title = {Balanced Information Gathering and Goal-Oriented Actions in Shared Autonomy},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Robotic teleoperation can be a complex task due to factors such as high degree-of-freedom manipulators, operator inexperience, and limited operator situational awareness. To reduce teleoperation complexity, researchers have developed the shared autonomy control paradigm that involves joint control of a robot by a human user and an autonomous control system. We introduce the concept of active learning into shared autonomy by developing a method for systems to leverage information gathering: minimizing the system's uncertainty about user goals by moving to information-rich states to observe user input. We create a framework for balancing information gathering actions, which help the system gain information about user goals, with goal-oriented actions, which move the robot towards the goal the system has inferred from the user. We conduct an evaluation within the context of users who are multitasking that compares pure teleoperation with two forms of shared autonomy: our balanced system and a traditional goal-oriented system. Our results show significant improvements for both shared autonomy systems over pure teleoperation in terms of belief convergence about the user's goal and task completion speed and reveal tradeoffs across shared autonomy strategies that may inform future investigations in this space.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {85–94},
numpages = {10},
keywords = {human-robot teaming, shared autonomy, active learning, inverse reinforcement learning, assistive teleoperation},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378696,
author = {Kshirsagar, Alap and Dreyfuss, Bnaya and Ishai, Guy and Heffetz, Ori and Hoffman, Guy},
title = {Monetary-Incentive Competition between Humans and Robots: Experimental Results},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In a controlled experiment, participants (n = 60) competed in a monotonous task with an autonomous robot for real monetary incentives. For each participant, we manipulated the robot's performance and the monetary incentive level across ten rounds. In each round, a participant's performance compared to the robot's would affect their odds in a lottery for the monetary prize. Standard economic theory predicts that people's effort will increase with prize value. Furthermore, recent work in behavioral economics predicts that there will also be a discouragement effect, with stronger robot performance discouraging human effort, and that this effect will increase with prize. We were not able to detect a meaningful effect of monetary prize, but we found a small discouragement effect, with human effort decreasing with increased robot performance, significant at the p &lt; 0.005 level. Using per-round subjective indicators, we also found a positive effect of robot performance on its perceived competence, a negative effect on the participants' liking of the robot, and a negative effect on the participants' own competence, all at p &lt; 0.0001. These findings shed light on how people may exert work effort and perceive robotic competitors in a human-robot workforce, and could have implications on labor supply decisions and the design of compensation schemes in the workplace.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {95–103},
numpages = {9},
keywords = {loss aversion, perceived competence, human-robot competition, reference-dependent preferences},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378697,
author = {Fraune, Marlena R. and Sherrin, Steven and \v{S}abanovi\'{c}, Selma and Smith, Eliot R.},
title = {Is Human-Robot Interaction More Competitive between Groups than between Individuals?},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {As robots, both individually and in groups, become more prevalent in everyday contexts (e.g., schools, workplaces, educational and caregiving institutions), it is possible that they will be perceived as outgroups, or come into competition for resources with humans. Research indicates that some of the psychological effects of intergroup interaction common in humans translate to human-robot interaction (HRI). In this paper, we examine how intergroup competition, like that among humans, translates to HRI. Specifically, we examined how Number of Humans (1, 3) and Number of Robots (1, 3) affect behavioral competition on dilemma tasks and survey ratings of perceived threat, emotion, and motivation (fear, greed, and outperformance). We also examined the effect of perceived group entitativity (i.e., cohesiveness) on competition motivation. Like in social psychological literature, these results indicate that groups of humans (especially entitative groups) showed more greed-based motivation and competition toward robots than individual humans did. However, we did not find evidence that number of robots had an effect on fear-based motivation or competition against them unless the robot groups were perceived as highly entitative. Our data also show the intriguing finding that participants displayed more fear of and competed slightly more against robots that matched their number. Future research should more deeply examine this novel pattern of results compared to one-on-one HRI and typical group dynamics in social psychology.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {104–113},
numpages = {10},
keywords = {social robotics, outperformance, competition, robots in society, discontinuity effect, group effects, groups of robots, fear, greed},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378699,
author = {Tan, Xiang Zhi and Reig, Samantha and Carter, Elizabeth J. and Steinfeld, Aaron},
title = {From One to Another: How Robot-Robot Interaction Affects Users' Perceptions Following a Transition between Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Human-robot interactions that involve multiple robots are becoming common. It is crucial to understand how multiple robots should transfer information and transition users between them. To investigate this, we designed a 3 \texttimes{} 3 mixed-design study in which participants took part in a navigation task. Participants interacted with a stationary robot who summoned a functional (not explicitly social) mobile robot to guide them. Each participant experienced the three types of robot-robot interaction: representative (the stationary robot spoke to the participant on behalf of the mobile robot), direct (the stationary robot delivered the request to the mobile robot in a straightforward manner), and social (the stationary robot delivered the request to the mobile robot in a social manner). Each participant witnessed only one type of robot-robot communication: silent (the robots covertly communicated), explicit (the robots acknowledged that they were communicating), or reciting (the stationary robot said the request aloud). Our results show that it is possible to instill socialness in and improve likability of a functional robot by having a social robot interact socially with it. We also found that covertly exchanging information is less desirable than reciting information aloud.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {114–122},
numpages = {9},
keywords = {multi-robot, human-robot interaction, transfer, transition, user study, robots, multi-robot-human interaction},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378700,
author = {Oliveira, Raquel and Arriaga, Patr\'{\i}cia and Correia, Filipa and Paiva, Ana},
title = {The Stereotype Content Model Applied to Human-Robot Interactions in Groups},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In this paper we sought to understand how the display of different levels of warmth and competence, as well as, different roles (opponent versus partner) portrayed by a robot, affect the display of emotional responses towards robots and how they can be used to predict future intention to work. For this purpose we devised an entertainment card-game group scenario involving two humans and two robots (n=54). The results suggest that different levels of warmth and competence are associated with distinct emotional responses from users and that these variables are useful in predicting future intention to work, thus hinting at the importance of considering warmth and competence stereotypes in Human-Robot Interaction.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {123–132},
numpages = {10},
keywords = {autonomous robots, emotions, human-robot interaction, stereotypes},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378701,
author = {Tennent, Hamish and Shen, Solace and Jung, Malte},
title = {Micbot: A Peripheral Robotic Object to Shape Conversational Dynamics and Team Performance},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Many of the problems we face are solved in small groups. Using decades of research from psychology, HRI research is increasingly trying to understand how robots impact the dynamics and outcomes of these small groups. Current work almost exclusively uses humanoid robots that take on the role of an active group participant to influence interpersonal dynamics. We argue that this has limitations and propose an alternative design approach of using a peripheral robotic object. This paper presents Micbot, a peripheral robotic object designed to promote participant engagement and ultimately performance using nonverbal implicit interactions. The robot is evaluated in a 3 condition (no movement, engagement behaviour, random movement) laboratory experiment with 36 three-person groups (N=108). Results showed that the robot was effective in promoting not only increased group engagement but also improved problem solving performance. In the engagement condition, participants displayed more even backchanneling toward one another, compared to no movement, but not to the random movement. This more even distribution of backchanneling predicted better problem solving performance.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {133–142},
numpages = {10},
keywords = {conversational dynamics, peripheral robot, team performance, interaction design, robot},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378702,
author = {Correia, Filipa and Mascarenhas, Samuel F. and Gomes, Samuel and Arriaga, Patr\'{\i}cia and Leite, Iolanda and Prada, Rui and Melo, Francisco S. and Paiva, Ana},
title = {Exploring Prosociality in Human-Robot Teams},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper explores the role of prosocial behaviour when people team up with robots in a collaborative game that presents a social dilemma similar to a public goods game. An experiment was conducted with the proposed game in which each participant joined a team with a prosocial robot and a selfish robot. During 5 rounds of the game, each player chooses between contributing to the team goal (cooperate) or contributing to his individual goal (defect). The prosociality level of the robots only affects their strategies to play the game, as one always cooperates and the other always defects. We conducted a user study at the office of a large corporation with 70 participants where we manipulated the game result (winning or losing) in a between-subjects design. Results revealed two important considerations: (1) the prosocial robot was rated more positively in terms of its social attributes than the selfish robot, regardless of the game result; (2) the perception of competence, the responsibility attribution (blame/credit), and the preference for a future partner revealed significant differences only in the losing condition. These results yield important concerns for the creation of robotic partners, the understanding of group dynamics and, from a more general perspective, the promotion of a prosocial society.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {143–151},
numpages = {9},
keywords = {social dilemma, prosocial, groups, selfish, public goods game},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378706,
author = {Cauchard, Jessica R. and Tamkin, Alex and Wang, Cheng Yao and Vink, Luke and Park, Michelle and Fang, Tommy and Landay, James A.},
title = {Drone.Io: A Gestural and Visual Interface for Human-Drone Interaction},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Drones are becoming ubiquitous and offer support to people in various tasks, such as photography, in increasingly interactive social contexts. We introduce drone.io, a projected body-centric graphical user interface for human-drone interaction. Using two simple gestures, users can interact with a drone in a natural manner. drone.io is the first human-drone graphical user interface embedded on a drone to provide both input and output capabilities. This paper describes the design process of drone.io. We present a proof of concept, drone-based implementation, as well as a fully functional prototype for a drone tour-guide scenario. We report drone.io's evaluation in three user studies (N=27) and show that people were able to use the interface with little prior training. We contribute to the field of human-robot interaction and the growing field of human-drone interaction.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {153–162},
numpages = {10},
keywords = {radial menus, human-drone interaction, UAV, robotics, mid-air gestures, mobile projector},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378707,
author = {Firestone, Justin W. and Qui\~{n}ones, Rubi and Duncan, Brittany A.},
title = {Learning from Users: An Elicitation Study and Taxonomy for Communicating Small Unmanned Aerial System States through Gestures},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper presents a gesture set for communicating states to novice users from a small Unmanned Aerial System (sUAS) through an elicitation study comparing gestures created by participants recruited from the general public with varying levels of experience with an sUAS. Previous work in sUAS flight paths sought to communicate intent, destination, or emotion without focusing on concrete states such as Low Battery or Landing. This elicitation study uses a participatory design approach from human-computer interaction to understand how novice users would expect an sUAS to communicate states, and ultimately suggests flight paths and characteristics to indicate those states. We asked users from the general public (N=20) to create gestures for seven distinct sUAS states to provide insights for human-drone interactions and to present intuitive flight paths and characteristics with the expectation that the sUAS would have general commercial application for inexperienced users. The results indicate relatively strong agreement scores for three sUAS states: Landing (0.455), Area of Interest (0.265), and Low Battery (0.245). The other four states have lower agreement scores, however even they show some consensus for all seven states. The agreement scores and the associated gestures suggest guidance for engineers to develop a common set of flight paths and characteristics for an sUAS to communicate states to novice users.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {163–171},
numpages = {9},
keywords = {elicitation study, sUAS, user design, communication},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378708,
author = {Wojciechowska, Anna and Frey, Jeremy and Sass, Sarit and Shafir, Roy and Cauchard, Jessica R.},
title = {Collocated Human-Drone Interaction: Methodology and Approach Strategy},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {The consumer drone market has shown a constant growth for the past few years. As drones become increasingly autonomous and used for a growing number of applications, it is crucial to establish parameters for collocated human-drone interaction. Prior research showed how ground robots should approach a person to initiate interaction. This paper builds upon prior work and investigates how a flying robot should approach a person. Because of the flight capability, drones present more approach parameters than ground robots and require further study to properly design future interactions. Since research methodologies in aerial robotics are not well established, we present a taxonomy of methodologies for human-drone interaction studies to guide future researchers in the field. This paper then contributes a user study (N=24) investigating proximity, speed, direction, and trajectory towards a comfortable drone approach. We present our study results and design guidelines for the safe approach of a drone in a collocated indoor environment.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {172–181},
numpages = {10},
keywords = {social robotics, proxemics, methodology survey, human-robot interaction, human-drone interaction},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378709,
author = {Sportillo, Daniele and Paljic, Alexis and Ojeda, Luciano},
title = {On-Road Evaluation of Autonomous Driving Training},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {182–190},
numpages = {9},
keywords = {human-vehicle interaction, transfer of training, TOR, augmented reality, virtual reality, automated vehicles, wizard of Oz},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378710,
author = {Williams, Tom and Bussing, Matthew and Cabrol, Sebastian and Boyle, Elizabeth and Tran, Nhan},
title = {Mixed Reality Deictic Gesture for Multi-Modal Robot Communication},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In previous work, researchers have repeatedly demonstrated that robots' use of deictic gestures enables effective and natural human-robot interaction. However, new technologies such as augmented reality head mounted displays enable environments in which mixed-reality becomes possible, and in such environments, physical gestures become but one category among many different types of mixed reality deictic gestures. In this paper, we present the first experimental exploration of the effectiveness of mixed reality deictic gestures beyond physical gestures. Specifically, we investigate human perception of videos simulating the display of allocentric gestures, in which robots circle their targets in users' fields of view. Our results suggest that this is an effective communication strategy, both in terms of objective accuracy and subjective perception, especially when paired with complex natural language references.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {191–201},
numpages = {11},
keywords = {deixis, human-robot interaction, mixed reality, natural language generation, augmented reality},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378711,
author = {Walker, Michael E. and Hedayati, Hooman and Szafir, Daniel},
title = {Robot Teleoperation with Augmented Reality Virtual Surrogates},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Teleoperation remains a dominant control paradigm for human interaction with robotic systems. However, teleoperation can be quite challenging, especially for novice users. Even experienced users may face difficulties or inefficiencies when operating a robot with unfamiliar and/or complex dynamics, such as industrial manipulators or aerial robots, as teleoperation forces users to focus on low-level aspects of robot control, rather than higher level goals regarding task completion, data analysis, and problem solving. We explore how advances in augmented reality (AR) may enable the design of novel teleoperation interfaces that increase operation effectiveness, support the user in conducting concurrent work, and decrease stress. Our key insight is that AR may be used in conjunction with prior work on predictive graphical interfaces such that a teleoperator controls a virtual robot surrogate, rather than directly operating the robot itself, providing the user with foresight regarding where the physical robot will end up and how it will get there. We present the design of two AR interfaces using such a surrogate: one focused on real-time control and one inspired by waypoint delegation. We compare these designs against a baseline teleoperation system in a laboratory experiment in which novice and expert users piloted an aerial robot to inspect an environment and analyze data. Our results revealed that the augmented reality prototypes provided several objective and subjective improvements, demonstrating the promise of leveraging AR to improve human-robot interactions.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {202–210},
numpages = {9},
keywords = {ARHMD, mixed reality, drones, augmented reality, aerial robots, robots, interface design, teleoperation},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378713,
author = {McGinn, Conor and Torre, Ilaria},
title = {Can You Tell the Robot by the Voice? An Exploratory Study on the Role of Voice in the Perception of Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {It is well established that a robot's visual appearance plays a significant role in how it is perceived. Considerable time and resources are usually dedicated to help ensure that the visual aesthetics of social robots are pleasing to users and helps facilitate clear communication. However, relatively little consideration is given to how the voice of the robot should sound, which may have adverse effects on acceptance and clarity of communication. In this study, we explore the mental images people form when they hear robots speaking. In our experiment, participants listened to several voices, and for each voice they were asked to choose a robot, from a selection of eight commonly used social robot platforms, that was best suited to have that voice. The voices were manipulated in terms of naturalness, gender, and accent. Results showed that a) participants seldom matched robots with the voices that were used in previous HRI studies, b) the gender and naturalness vocal manipulations strongly affected participants' selection, and c) the linguistic content of the utterances spoken by the voices does not affect people's selection. This finding suggests that people associate voices with robot pictures, even when the content of spoken utterances was unintelligible. Our findings indicate that both a robot's voice and its appearance contribute to robot perception. Thus, giving a mismatched voice to a robot might introduce a confounding effect in HRI studies. We therefore suggest that voice design should be considered more thoroughly when planning spoken human-robot interactions.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {211–221},
numpages = {11},
keywords = {mental model, voice, speech, robot design},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378714,
author = {Mieczkowski, Hannah and Liu, Sunny Xun and Hancock, Jeffrey and Reeves, Byron},
title = {Helping Not Hurting: Applying the Stereotype Content Model and BIAS Map to Social Robotics},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper examines relationships between perceptions of warmth and competence, emotional responses, and behavioral tendencies in the context of social robots. Participants answered questions about these three aspects of impression formation after viewing an image of one of 342 social robots in the Stanford Social Robots Database. Results suggest that people have similar emotional and behavioral reactions to robots as they have to humans; impressions of the robots' warmth and competence predicted specific emotional responses (admiration, envy, contempt, pity) and those emotional responses predicted distinct behavioral tendencies (active facilitation, active harm, passive facilitation, passive harm). However, the predicted relationships between impressions and harmful behavioral tendencies were absent. This novel asymmetry for perceptions and intentions towards robots is deliberated in the context of the computers as social actors framework and opportunities for further research are discussed.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {222–229},
numpages = {8},
keywords = {social robots, behaviors, cognitions, emotions},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378715,
author = {Chita-Tegmark, Meia and Lohani, Monika and Scheutz, Matthias},
title = {Gender Effects in Perceptions of Robots and Humans with Varying Emotional Intelligence},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Robots are machines and as such do not have gender. However, many of the gender-related perceptions and expectations formed in human-human interactions may be inadvertently and unreasonably transferred to interactions with social robots. In this paper, we investigate how gender effects in people's perception of robots and humans depend on their emotional intelligence (EI), a crucial component of successful human social interactions. Our results show that participants perceive different levels of EI in robots just as they do in humans. Also, their EI perceptions are affected by gender-related expectations both when judging humans and when judging robots with minimal gender markers, such as voice or even just a name. We discuss the implications for human-robot interactions (HRI) and propose further explorations of EI for future HRI studies.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {230–238},
numpages = {9},
keywords = {human robot interaction, emotional intelligence, social robots, gender},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378716,
author = {de Graaf, Maartje M. A. and Malle, Bertram F.},
title = {People's Explanations of Robot Behavior Subtly Reveal Mental State Inferences},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {It has long been assumed that when people observe robots they intuitively ascribe mind and intentionality to them, just as they do to humans. However, much of this evidence relies on experimenter-provided questions or self-reported judgments. We propose a new way of investigating people's mental state ascriptions to robots by carefully studying explanations of robot behavior. Since people's explanations of human behavior are deeply grounded in assumptions of mind and intentional agency, explanations of robot behavior can reveal whether such assumptions similarly apply to robots. We designed stimulus behaviors that were representative of a variety of robots in diverse contexts and ensured that people saw the behaviors as equally intentional, desirable, and surprising across both human and robot agents. We provided 121 participants with verbal descriptions of these behaviors and asked them to explain in their own words why the agent (human or robot) had performed them. To systematically analyze the verbal data, we used a theoretically grounded classification method to identify core explanation types. We found that people use the same conceptual toolbox of behavior explanations for both human and robot agents, robustly indicating inferences of intentionality and mind. But people applied specific explanatory tools at somewhat different rates and in somewhat different ways for robots, revealing specific expectations people hold when explaining robot behaviors.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {239–248},
numpages = {10},
keywords = {human-robot-interaction, theory of mind, folk psychology, mental state inference, behavior explanation},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378717,
author = {Tabrez, Aaquib and Agrawal, Shivendra and Hayes, Bradley},
title = {Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {249–257},
numpages = {9},
keywords = {policy explanation, joint task execution, explainable AI, human-robot collaboration, reward estimation},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378718,
author = {Chakraborti, Tathagata and Sreedharan, Sarath and Grover, Sachin and Kambhampati, Subbarao},
title = {Plan Explanations as Model Reconciliation: An Empirical Study},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Recent work in explanation generation for decision making agents has looked at how unexplained behavior of autonomous systems can be understood in terms of differences in the model of the system and the human's understanding of the same, and how the explanation process as a result of this mismatch can be then seen as a process of reconciliation of these models. Existing algorithms in such settings, while having been built on contrastive, selective and social properties of explanations as studied extensively in the psychology literature, have not, to the best of our knowledge, been evaluated in settings with actual humans in the loop. As such, the applicability of such explanations to human-AI and human-robot interactions remains suspect. In this paper, we set out to evaluate these explanation generation algorithms in a series of studies in a mock search and rescue scenario with an internal semi-autonomous robot and an external human commander. During that process, we hope to demonstrate to what extent the properties of these algorithms hold as they are evaluated by humans.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {258–266},
numpages = {9},
keywords = {human-robot interaction, explanations as model reconciliation, planning and decision-making, explainable AI},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378720,
author = {Gallenberger, Daniel and Bhattacharjee, Tapomayukh and Kim, Youngsun and Srinivasa, Siddhartha S.},
title = {Transfer Depends on Acquisition: Analyzing Manipulation Strategies for Robotic Feeding},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Successful robotic assistive feeding depends on reliable bite acquisition and easy bite transfer. The latter constitutes a unique type of robot-human handover where the human needs to use the mouth. This places a high burden on the robot to make the transfer easy. We believe that the ease of transfer not only depends on the transfer action but also is tightly coupled with the way a food item was acquired in the first place. To determine the factors influencing good bite transfer, we designed both skewering and transfer primitives and developed a robotic feeding system that uses these manipulation primitives to feed people autonomously. First, we determined the primitives' success rates for bite acquisition with robot experiments. Next, we conducted user studies to evaluate the ease of bite transfer for different combinations of skewering and transfer primitives. Our results show that an intelligent food item dependent skewering strategy improves the bite acquisition success rate and that the choice of skewering location and the fork orientation affects the ease of bite transfer significantly.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {267–276},
numpages = {10},
keywords = {bite transfer, assistive feeding, deformable object manipulation, bite acquisition},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378721,
author = {Winkle, Katie and Lemaignan, S\'{e}verin and Caleb-Solly, Praminda and Leonards, Ute and Turton, Ailie and Bremner, Paul},
title = {Effective Persuasion Strategies for Socially Assistive Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In this paper we present the results of an experimental study investigating the application of human persuasive strategies to a social robot. We demonstrate that robot displays of goodwill and similarity to the participant significantly increased robot persuasiveness, as measured objectively by participant behaviour. However, such strategies had no impact on subjective measures concerning perception of the robot, and perception of the robot did not correlate with participant behaviour. We hypothesise that this is due to difficulty in accurately measuring perception of a robot using subjective measures. We suggest our results are particularly relevant for the design and development of socially assistive robots.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {277–285},
numpages = {9},
keywords = {user-study, persuasion, socially assistive robots},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378722,
author = {Petric, Frano and Kova\v{c}i\'{c}, Zdenko},
title = {Hierarchical POMDP Framework for a Robot-Assisted ASD Diagnostic Protocol},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Since the diagnosis of autism spectrum disorder (ASD) relies heavily on behavioral observations by experienced clinician, we seek to investigate whether parts of this job can be autonomously performed by a humanoid robot using only sensors available on-board. To that end, we developed a robot-assisted ASD diagnostic protocol. In this work we propose the Partially observable Markov decision process (POMDP) framework for such protocol which enables the robot to infer information about the state of the child based on observations of child's behavior. We extend our previous work by developing a protocol POMDP model which uses tasks of the protocol as actions. We devise a method to interface protocol and task models by using belief at the end of a task to generate observations for the protocol POMDP, resulting in a hierarchical POMDP framework. We evaluate our approach through an exploratory study with fifteen children (seven typically developing and eight with ASD).},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {286–293},
numpages = {8},
keywords = {robot-assisted, autism, diagnostics, NAO, POMDP},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378723,
author = {Utami, Dina and Bickmore, Timothy},
title = {Collaborative User Responses in Multiparty Interaction with a Couples Counselor Robot},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Intimate relationships are integral parts of human societies, yet many relationships are in distress. Couples counseling has been shown to be effective in preventing and alleviating relationship distress, yet many couples do not seek professional help, due to cost, logistic, and discomfort in disclosing private problems. In this paper, we describe our efforts towards the development a fully automated couples counselor robot, and focus specifically on the problem of identifying and processing "collaborative responses", in which a human couple co-construct a response to a query from the robot. We present an analysis of collaborative responses obtained from a pilot study, then develop a data-driven model to detect end of collaborative responses for regulating turn taking during a counseling session. Our model uses a combination of multimodal features, and achieves an offline weighted F-score of 0.81. Finally, we present findings from a quasi-experimental study with a robot facilitating a counseling session to promote intimacy with romantic couples. Our findings suggest that the session improves couples intimacy and positive affect. An online evaluation of the end-of-collaborative-response model demonstrates an F-score of 0.72.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {294–303},
numpages = {10},
keywords = {human-robot interaction, multiparty interaction, turn taking, collaborative responses, user studies},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378746,
author = {Roesler, Oliver and Aly, Amir and Taniguchi, Tadahiro and Hayashi, Yoshikatsu},
title = {Evaluation of Word Representations in Grounding Natural Language Instructions through Computational Human-Robot Interaction},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In order to interact with people in a natural way, a robot must be able to link words to objects and actions. Although previous studies in the literature have investigated grounding, they did not consider grounding of unknown synonyms. In this paper, we introduce a probabilistic model for grounding unknown synonymous object and action names using cross-situational learning. The proposed Bayesian learning model uses four different word representations to determine synonymous words. Afterwards, they are grounded through geometric characteristics of objects and kinematic features of the robot joints during action execution. The proposed model is evaluated through an interaction experiment between a human tutor and HSR robot. The results show that semantic and syntactic information both enable grounding of unknown synonyms and that the combination of both achieves the best grounding.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {307–316},
numpages = {10},
keywords = {computational human-robot interaction, Bayesian learning model, language grounding, cross-situational learning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378747,
author = {Choudhury, Rohan and Swamy, Gokul and Hadfield-Menell, Dylan and Dragan, Anca D.},
title = {On the Utility of Model Learning in HRI},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a "theory of mind" to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot's theory of mind for the human are wrong, as they inevitably will be in practice. We find that there is a significant sample complexity advantage to theory of mind methods and that they are more robust to covariate shift, but that when enough interaction data is available, black box approaches eventually dominate.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {317–325},
numpages = {9},
keywords = {theory of mind, inverse RL, sample complexity, model-based RL, model-free RL},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378748,
author = {Jackson, Astrid and Northcutt, Brandon D. and Sukthankar, Gita},
title = {The Benefits of Immersive Demonstrations for Teaching Robots},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {One of the advantages of teaching robots by demonstration is that it can be more intuitive for users to demonstrate rather than describe the desired robot behavior. However, when the human demonstrates the task through an interface, the training data may inadvertently acquire artifacts unique to the interface, not the desired execution of the task. Being able to use one's own body usually leads to more natural demonstrations, but those examples can be more difficult to translate to robot control policies.This paper quantifies the benefits of using a virtual reality system that allows human demonstrators to use their own body to perform complex manipulation tasks. We show that our system generates superior demonstrations for a deep neural network without introducing a correspondence problem. The effectiveness of this approach is validated by comparing the learned policy to that of a policy learned from data collected via a conventional gaming system, where the user views the environment on a monitor screen, using a Sony Play Station 3 (PS3) DualShock 3 wireless controller as input.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {326–334},
numpages = {9},
keywords = {virtual reality, robot manipulation, user study, learning from demonstration, imitation learning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378749,
author = {Racca, Mattia and Oulasvirta, Antti and Kyrki, Ville},
title = {Teacher-Aware Active Robot Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper investigates Active Robot Learning strategies that take into account the effort of the user in an interactive learning scenario. Most research claims that Active Learning's sample efficiency can reduce training time and therefore the effort of the human teacher. We argue that the performance driven query selection of standard Active Learning can make the job of the human teacher difficult, resulting in a decrease in training quality due to slowdowns or increased error rates. We investigate this issue by proposing a learning strategy that aims to minimize the user's workload by taking into account the flow of the questions. We compare this strategy against a standard Active Learning strategy based on uncertainty sampling and a third strategy being an hybrid of the two. After studying in simulation the validity and the behavior of these approaches, we conducted a user study where 26 subjects interacted with a NAO robot embodying the presented strategies. We reports results from both the robot's performance and the human teacher's perspectives, observing how the hybrid strategy represents a good compromise between learning performance and user's experienced workload. Based on the results, we provide recommendations on the development of Active Robot Learning strategies going beyond robot's performance.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {335–343},
numpages = {9},
keywords = {active learning, human-robot interaction, interactive machine learning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378750,
author = {Praveena, Pragathi and Subramani, Guru and Mutlu, Bilge and Gleicher, Michael},
title = {Characterizing Input Methods for Human-to-Robot Demonstrations},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Human demonstrations are important in a range of robotics applications, and are created with a variety of input methods. However, the design space for these input methods has not been extensively studied. In this paper, focusing on demonstrations of hand-scale object manipulation tasks to robot arms with two-finger grippers, we identify distinct usage paradigms in robotics that utilize human-to-robot demonstrations, extract abstract features that form a design space for input methods, and characterize existing input methods as well as a novel input method that we introduce, the instrumented tongs. We detail the design specifications for our method and present a user study that compares it against three common input methods: free-hand manipulation, kinesthetic guidance, and teleoperation. Study results show that instrumented tongs provide high quality demonstrations and a positive experience for the demonstrator while offering good correspondence to the target robot.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {344–353},
numpages = {10},
keywords = {input methods, human demonstrations, user experience, design space, robot programming},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378751,
author = {Chan, Lawrence and Hadfield-Menell, Dylan and Srinivasa, Siddhartha and Dragan, Anca},
title = {The Assistive Multi-Armed Bandit},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {354–363},
numpages = {10},
keywords = {assistive agents, preference learning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378753,
author = {Mavrogiannis, Christoforos and Hutchinson, Alena M. and Macdonald, John and Alves-Oliveira, Patr\'{\i}cia and Knepper, Ross A.},
title = {Effects of Distinct Robot Navigation Strategies on Human Behavior in a Crowded Environment},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {State-of-the-art social robot navigation algorithms often lack a thorough experimental validation in human environments: simulated evaluations are often conducted under unrealistically strong assumptions that prohibit deployment in real world environments; experimental demonstrations that are limited in sample size do not provide adequate evidence regarding the user experience and the robot behavior; field studies may suffer from the noise imposed by uncontrollable factors from the environment; controlled lab experiments often fail to properly enforce challenging interaction settings. This paper contributes a first step towards addressing the outlined gaps in the literature. We present an original experiment, designed to test the implicit interaction between a mobile robot and a group of navigating human participants, under challenging settings in a controlled lab environment. We conducted a large-scale, within-subjects design study with 105 participants, exposed to three different conditions, corresponding to three distinct navigation strategies, executed by a telepresence robot (two autonomous, one teleoperated). We analyzed observed human and robot trajectories, under close interaction settings and participants' impressions regarding the robot's behavior. Key findings, extracted from a comparative statistical analysis include: (1) evidence that human acceleration is lower when navigating around an autonomous robot compared to a teleoperated one; (2) the lack of evidence to support the conventional expectation that teleoperation would be humans' preferred strategy. To the best of our knowledge, our study is unique in terms of goals, settings, thoroughness of evaluation and sample size.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {421–430},
numpages = {10},
keywords = {navigation, social robotics, motion planning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378754,
author = {Li, Rui and van Almkerk, Marc and van Waveren, Sanne and Carter, Elizabeth and Leite, Iolanda},
title = {Comparing Human-Robot Proxemics between Virtual Reality and the Real World},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Virtual Reality (VR) can greatly benefit Human-Robot Interaction (HRI) as a tool to effectively iterate across robot designs. However, possible system limitations of VR could influence the results such that they do not fully reflect real-life encounters with robots. In order to better deploy VR in HRI, we need to establish a basic understanding of what the differences are between HRI studies in the real world and in VR. This paper investigates the differences between the real life and VR with a focus on proxemic preferences, in combination with exploring the effects of visual familiarity and spatial sound within the VR experience. Results suggested that people prefer closer interaction distances with a real, physical robot than with a virtual robot in VR. Additionally, the virtual robot was perceived as more discomforting than the real robot, which could result in the differences in proxemics. Overall, these results indicate that the perception of the robot has to be evaluated before the interaction can be studied. However, the results also suggested that VR settings with different visual familiarities are consistent with each other in how they affect HRI proxemics and virtual robot perceptions, indicating the freedom to study HRI in various scenarios in VR. The effect of spatial sound in VR drew a more complex picture and thus calls for more in-depth research to understand its influence on HRI in VR.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {431–439},
numpages = {9},
keywords = {human-robot interaction, proxemics, virtual reality},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378755,
author = {Han, Zhao and Yanco, Holly},
title = {The Effects of Proactive Release Behaviors during Human-Robot Handovers},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Most research on human-robot handovers focuses on how the robot should approach human receivers and notify them of the readiness to take an object; few studies have investigated the effects of different release behaviors. Not releasing an object when a person desires to take it breaks handover fluency and creates a bad handover experience. In this paper, we investigate the effects of different release behaviors. Specifically, we study the benefits of a proactive release, during which the robot actively detects a human grasp effort pattern. In a 36-participant user study1, results suggest proactive release is more efficient than rigid release (which only releases when the robot is fully stopped) and passive release (the robot detects pulling by checking if a threshold value is reached). Subjectively, the overall handover experience is improved: the proactive release is significantly better in terms of handover fluency and ease-of-taking.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {440–448},
numpages = {9},
keywords = {human-robot handover, robots},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378756,
author = {Mizumaru, Kazuki and Satake, Satoru and Kanda, Takayuki and Ono, Tetsuo},
title = {Stop Doing It! Approaching Strategy for a Robot to Admonish Pedestrians},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {We modeled a robot's approaching behavior for giving admonishment. We started by analyzing human behaviors. We conducted a data collection in which a guard approached others in two ways: 1) for admonishment, and 2) for a friendly purpose. We analyzed the difference between the admonishing approach and the friendly approach. The approaching trajectories in the two approaching types are similar; nevertheless, there are two subtle differences. First, the admonishing approach is slightly faster (1.3 m/sec) than the friendly approach (1.1 m/sec). Second, at the end of the approach, there is a 'shortcut' in the trajectory. We implemented this model of the admonishing approach into a robot. Finally, we conducted a field experiment to verify the effectiveness of the model. A robot is used to admonish people who were using a smartphone while walking. The result shows that significantly more people yield to admonishment from a robot using the proposed method than from a robot using the friendly approach method.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {449–457},
numpages = {9},
keywords = {approaching behavior, admonishment},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378757,
author = {Moharana, Sanika and Panduro, Alejandro E. and Lee, Hee Rin and Riek, Laurel D.},
title = {Robots for Joy, Robots for Sorrow: Community Based Robot Design for Dementia Caregivers},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Many new technologies are being built to support people with dementia. However, they largely focus on the people with dementia; consequently, informal caregivers, one of the most important stakeholders in dementia care, remain invisible within the technology design space. In this paper, we present a six-month long, community-based design research process where we collaborated with dementia caregiver support groups to design robots for dementia caregiving. The contributions of this paper are threefold. First, we broaden the context of dementia robot design to give a more prominent role to informal family caregivers in the co-design process. Second, we provide new design guidelines that contextualize robots within the family caregiving paradigm, which suggest new roles and behaviors of robots. These include lessening emotional labor by communicating information caregivees do not want to hear (e.g., regarding diet or medication) or providing redirection during emotionally difficult times, as well as facilitating positive shared moments. Third, our work found connections between certain robot attributes and their relationship to the stage of dementia a caregivee is experiencing. For example, caregivers wanted their robots to facilitate interaction with their caregivees in early stages of dementia, yet be in the background. However, for later stages of dementia, they wanted robots to replace caregiver-caregivee interaction to lessen their emotional burden, and be foregrounded. These connections provide important insights in to how we think about adaptability and long-term interaction in HRI. We hope our work provides new avenues for HRI researchers to studying robots for dementia caregivers by engaging in community-based design.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {458–467},
numpages = {10},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378758,
author = {Short, Elaine Schaertl and Allevato, Adam and Thomaz, Andrea L.},
title = {SAIL: Simulation-Informed Active in-the-Wild Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Robots in real-world environments may need to adapt context-specific behaviors learned in one environment to new environments with new constraints. In many cases, copresent humans can provide the robot with information, but it may not be safe for them to provide hands-on demonstrations and there may not be a dedicated supervisor to provide constant feedback. In this work we present the SAIL (Simulation-Informed Active In-the-Wild Learning) algorithm for learning new approaches to manipulation skills starting from a single demonstration. In this three-step algorithm, the robot simulates task execution to choose new potential approaches; collects unsupervised data on task execution in the target environment; and finally, chooses informative actions to show to co-present humans and obtain labels. Our approach enables a robot to learn new ways of executing two different tasks by using success/failure labels obtained from na\"{\i}ve users in a public space, performing 496 manipulation actions and collecting 163 labels from users in the wild over six 45-minute to 1-hour deployments. We show that classifiers based low-level sensor data can be used to accurately distinguish between successful and unsuccessful motions in a multi-step task (p &lt; 0.005), even when trained in the wild. We also show that using the sensor data to choose which actions to sample is more effective than choosing the least-sampled action.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {468–477},
numpages = {10},
keywords = {learning from demonstration, in-the-wild human-robot interaction, robot learning},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378760,
author = {Joshi, Swapna and \v{S}abanovi\'{c}, Selma},
title = {Robots for Inter-Generational Interactions: Implications for Nonfamilial Community Settings},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Social robots have been designed to engage with older adults and children separately, but their use for inter-generational (IG) interactions, especially in nonfamilial settings, has not been studied. In addition to the challenge of simultaneously meeting the varied needs and preferences of older adults and children, the dynamic nature of these settings makes the use of robots for IG activities difficult. This paper presents a first exploratory study meant to inform the design and use of social robots for IG activities in nonfamilial settings by analyzing interviews and observations conducted at a co-located preschool and assisted living-dementia care center. Interactions occurring with and around robots were analyzed, particularly focusing on whether they fulfill the community's goals of providing children and older adults with engaging opportunities for IG contact. Findings suggest integrating intermittent pauses and breaks in interactions with the robot and unstructured collaborative robot-assisted activities can meet the needs of both generations, and call for greater community involvement in HRI for IG research.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {478–486},
numpages = {9},
keywords = {inter-generational interactions, community robots, nonfamilial interactions, community-oriented HRI, social robots},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378761,
author = {Lee, Jin Joo and Sha, Fei and Breazeal, Cynthia},
title = {A Bayesian Theory of Mind Approach to Nonverbal Communication},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {This paper defines a dual computational framework to nonverbal communication for human-robot interactions. We use a Bayesian Theory of Mind approach to model dyadic storytelling interactions where the storyteller and the listener have distinct roles. The role of storytellers is to influence and infer the attentive state of listeners using speaker cues, and we computationally model this as a POMDP planning problem. The role of listeners is to convey attentiveness by influencing perceptions through listener responses, which we computational model as a DBN with a myopic policy. Through a comparison of state estimators trained on human-human interaction data, we validate our storyteller model by demonstrating how it outperforms current approaches to attention recognition. Then through a human-subjects experiment where children told stories to robots, we demonstrate that a social robot using our listener model more effectively communicates attention compared to alternative approaches based on signaling.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {487–496},
numpages = {10},
keywords = {speaker cues, theory of mind, attention recognition and expression, nonverbal behaviors, listener backchannels, DBN, POMDP, machine learning, children and storytelling},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378762,
author = {Vogt, Paul and van den Berghe, Rianne and de Haas, Mirjam and Hoffman, Laura and Kanero, Junko and Mamus, Ezgi and Montanier, Jean-Marc and Oran\c{c}, Cansu and Oudgenoeg-Paz, Ora and Garc\'{\i}a, Daniel Hern\'{a}ndez and Papadopoulos, Fotios and Schodde, Thorsten and Verhagen, Josje and Wallbridge, Christopher D. and Willemsen, Bram and de Wit, Jan and Belpaeme, Tony and G\"{o}ksun, Tilbe and Kopp, Stefan and Krahmer, Emiel and K\"{u}ntay, Aylin C. and Leseman, Paul and Pandey, Amit Kumar},
title = {Second Language Tutoring Using Social Robots: A Large-Scale Study},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {We present a large-scale study of a series of seven lessons designed to help young children learn English vocabulary as a foreign language using a social robot. The experiment was designed to investigate 1) the effectiveness of a social robot teaching children new words over the course of multiple interactions (supported by a tablet), 2) the added benefit of a robot's iconic gestures on word learning and retention, and 3) the effect of learning from a robot tutor accompanied by a tablet versus learning from a tablet application alone. For reasons of transparency, the research questions, hypotheses and methods were preregistered. With a sample size of 194 children, our study was statistically well-powered. Our findings demonstrate that children are able to acquire and retain English vocabulary words taught by a robot tutor to a similar extent as when they are taught by a tablet application. In addition, we found no beneficial effect of a robot's iconic gestures on learning gains.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {497–505},
numpages = {9},
keywords = {robots for learning, long-term interaction, second language tutoring, gesture, child-robot interaction},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.5555/3378680.3378763,
author = {Strait, Megan and Urry, Heather L. and Muentener, Paul},
title = {Children's Responding to Humanlike Agents Reflects an Uncanny Valley},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Both perceptual mechanisms (e.g., threat detection/avoidance) and social mechanisms (e.g., fears fostered via negative media) may explain the existence of the uncanny valley; however, existing literature lacks sufficient evidence to decide whether one, the other, or a combination best accounts for the valley's effects. As perceptually oriented explanations imply the valley should be evident early in development, we investigated whether it presents in the responding of children (N = 80; ages 5 --10) to agents of varying human similarity. We found that, like adults, children were most averse to highly humanlike robots (relative to less humanlike robots and humans). But, unlike adults, children's aversion did not translate to avoidance. The findings thus indicate, consistent with perceptual explanations, that the valley effect manifests well before adulthood. However, further research is needed to understand the emergence of the valley's behavioral consequences.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {506–515},
numpages = {10},
keywords = {agency, anthropomorphism, child-robot interaction, uncanny valley, cognitive development, humanoids},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

